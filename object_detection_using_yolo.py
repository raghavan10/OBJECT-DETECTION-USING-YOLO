# -*- coding: utf-8 -*-
"""Object Detection using YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTZ-nknpLg7rb20-vNDFA7wHml6o76At
"""

!nvidia-smi

"""## Install"""

!pip install -q supervision ultralytics

"""## Imports"""

import cv2

import numpy as np
import supervision as sv

from tqdm import tqdm
from ultralytics import YOLO
from supervision.assets import VideoAssets, download_assets
from collections import defaultdict, deque

"""## Download Data

**NOTE:** In this notebook we will use one of the Supervision Assets videos. [Here](https://supervision.roboflow.com/assets) you can learn more about it.
"""

# download_assets(VideoAssets.VEHICLES)

!pip install --upgrade supervision

"""## Configuration"""

SOURCE_VIDEO_PATH = "traffic2.mp4"
TARGET_VIDEO_PATH = "result.mp4"
CONFIDENCE_THRESHOLD = 0.5
IOU_THRESHOLD = 0.4

MODEL_NAME = "yolov8x.pt"
MODEL_RESOLUTION = 1280

"""## Source and Target ROIs

![Source and Target ROIs](https://storage.googleapis.com/com-roboflow-marketing/notebooks/speed-estimation-perspective-1.png)
"""

SOURCE = np.array([
    [570,280],
    [1330,280],
    [3500, 1050],
    [-1650, 1050]
])

TARGET_WIDTH = 51
TARGET_HEIGHT = 80

TARGET = np.array([
    [0, 0],
    [TARGET_WIDTH - 1, 0],
    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],
    [0, TARGET_HEIGHT - 1],
])

CLASS_NAMES_DICT = {
    0: 'person',
    1: 'bicycle',
    2: 'car',
    3: 'motorcycle',
    5: 'bus',
    7: 'truck'
}

frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)
frame_iterator = iter(frame_generator)
frame = next(frame_iterator)

annotated_frame = frame.copy()
annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.RED, thickness=4)
sv.plot_image(annotated_frame)

"""## Transform Perspective"""

class ViewTransformer:

    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:
        source = source.astype(np.float32)
        target = target.astype(np.float32)
        self.m = cv2.getPerspectiveTransform(source, target)

    def transform_points(self, points: np.ndarray) -> np.ndarray:
        if points.size == 0:
            return points

        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)
        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)
        return transformed_points.reshape(-1, 2)

view_transformer = ViewTransformer(source=SOURCE, target=TARGET)

# Load the model
model = YOLO(MODEL_NAME)

# Video info and frame generator
video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)
frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)

# Tracker initiation
byte_track = sv.ByteTrack(
    frame_rate=video_info.fps, track_activation_threshold=CONFIDENCE_THRESHOLD
)

# Annotators configuration
thickness = sv.calculate_optimal_line_thickness(
    resolution_wh=video_info.resolution_wh
)
text_scale = sv.calculate_optimal_text_scale(
    resolution_wh=video_info.resolution_wh
)
bounding_box_annotator = sv.BoxAnnotator(
    thickness=thickness
)
label_annotator = sv.LabelAnnotator(
    text_scale=text_scale,
    text_thickness=thickness,
    text_position=sv.Position.BOTTOM_CENTER
)
trace_annotator = sv.TraceAnnotator(
    thickness=thickness,
    trace_length=video_info.fps * 1,
    position=sv.Position.BOTTOM_CENTER
)

# Define polygon zone
polygon_zone = sv.PolygonZone(
    polygon=SOURCE,
    frame_resolution_wh=video_info.resolution_wh
)

coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))

# Open target video
with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
    # Loop over source video frames
    for frame in tqdm(frame_generator, total=video_info.total_frames):
        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]
        detections = sv.Detections.from_ultralytics(result)

        # Filter detections by confidence and class
        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]
        detections = detections[detections.class_id != 0]

        # Filter detections outside the zone
        detections = detections[polygon_zone.trigger(detections)]

        # Refine detections using non-max suppression
        detections = detections.with_nms(IOU_THRESHOLD)

        # Pass detection through the tracker
        detections = byte_track.update_with_detections(detections=detections)

        # Get detection points
        points = detections.get_anchors_coordinates(
            anchor=sv.Position.BOTTOM_CENTER
        )

        # Transform detection points
        points = view_transformer.transform_points(points=points).astype(int)

        # Store detection positions
        for tracker_id, [_, y] in zip(detections.tracker_id, points):
            coordinates[tracker_id].append(y)

        # Format labels
        labels = []
        for tracker_id, class_id, confidence in zip(detections.tracker_id, detections.class_id, detections.confidence):
            class_name = CLASS_NAMES_DICT.get(class_id, 'unknown')
            if len(coordinates[tracker_id]) < video_info.fps / 2:
                labels.append(f"#{tracker_id} {class_name} {confidence:.2f}")
            else:
                # Calculate speed
                coordinate_start = coordinates[tracker_id][-1]
                coordinate_end = coordinates[tracker_id][0]
                distance = abs(coordinate_start - coordinate_end)
                time = len(coordinates[tracker_id]) / video_info.fps
                speed = distance / time * 3.6
                labels.append(f"#{tracker_id} {class_name} {int(speed)} km/h")

        # Annotate frame
        annotated_frame = frame.copy()
        annotated_frame = trace_annotator.annotate(
            scene=annotated_frame, detections=detections
        )
        annotated_frame = bounding_box_annotator.annotate(
            scene=annotated_frame, detections=detections
        )
        annotated_frame = label_annotator.annotate(
            scene=annotated_frame, detections=detections, labels=labels
        )

        # Add frame to target video
        sink.write_frame(annotated_frame)

!pip install gradio

import gradio as gr
import cv2
import tempfile
import shutil

# Function to play video
def play_video(video_file):
    # OpenCV to read the video
    cap = cv2.VideoCapture(video_file)

    # Check if video opened successfully
    if not cap.isOpened():
        return "Error: Could not open video."

    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Temporary directory to save frames
    temp_dir = tempfile.mkdtemp()

    frame_list = []
    frame_num = 0

    # Read until video is completed
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
            # Save frame as image file
            frame_path = f"{temp_dir}/frame_{frame_num:04d}.png"
            cv2.imwrite(frame_path, frame)
            frame_list.append(frame_path)
            frame_num += 1
        else:
            break

    # Release the video capture object
    cap.release()

    return gr.update(video=frame_list, fps=fps)

# Create Gradio interface
interface = gr.Interface(
    fn=play_video,
    inputs=gr.Video('traffic2.mp4'),
    outputs=gr.Video('result.mp4'),
    title="Video Player",
    description="Result:"
)

# Launch the interface
interface.launch()

import gradio as gr
import cv2
import tempfile
import os

# Function to play video
def play_video(video_file):
    # OpenCV to read the video
    cap = cv2.VideoCapture(video_file)

    # Check if video opened successfully
    if not cap.isOpened():
        return "Error: Could not open video."

    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    # Desired output size
    target_width = 1080
    target_height = 720

    # Temporary directory to save frames
    temp_dir = tempfile.mkdtemp()

    # Path for the output video file
    output_video_path = os.path.join(temp_dir, "output.mp4")

    # Video writer to save resized frames
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (target_width, target_height))

    # Read until video is completed
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
            # Resize frame
            resized_frame = cv2.resize(frame, (target_width, target_height))
            # Write resized frame to video
            out.write(resized_frame)
        else:
            break

    # Release the video capture and writer objects
    cap.release()
    out.release()

    return output_video_path

# Create Gradio interface
interface = gr.Interface(
    fn=play_video,
    inputs=gr.Video('traffic2.mp4'),
    outputs=gr.Video('result.mp4',label="Resized Video Playback"),
    title="Video Player",
    description="Upload a video file to resize and play.",
    live=True,
    theme='compact',
    css=".block .input_panel {display: none} .block .output_panel {display: none} .block .btn_submit {display: none} "
)

# Launch the interface
interface.launch()